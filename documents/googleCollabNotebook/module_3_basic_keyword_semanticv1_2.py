# -*- coding: utf-8 -*-
"""Module-3-basic-keyword-semanticv1.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174mETJ2i88vi2JcugKAaS5RkmW7XTnTK
"""

# Import the Natural Language Toolkit (nltk) library
import nltk

# Download the Punkt tokenizer models
nltk.download('punkt')
# The 'punkt' tokenizer models are used for tokenizing text into words or sentences.
# This is a pre-trained model provided by NLTK that helps in splitting text into individual tokens (words or punctuation).

# Import the word_tokenize function from nltk.tokenize module
from nltk.tokenize import word_tokenize
# `word_tokenize` is a function that splits a given text into a list of words and punctuation.
# It uses the Punkt tokenizer model to perform tokenization.

# Import the math module
import math
# The `math` module provides mathematical functions and constants.
# It includes functions for mathematical operations such as logarithms, trigonometric functions, and constants like pi.

# Import the spatial module from scipy
from scipy import spatial
# The `spatial` module from the SciPy library provides functions for spatial operations and distance calculations.
# It includes methods for computing various distance metrics such as Euclidean distance, cosine distance, etc.

#!pip install sentence_transformers openai unstructured

"""Let's discuss why we need chunking/ need to divide data - context length"""

txt = '''I would like to get your all  thoughts on the bond yield increase this week.  I am not worried about the market downturn but the sudden increase in yields. On 2/16 the 10 year bonds yields increased by almost  9 percent and on 2/19 the yield increased by almost 5 percent.

Key Points from the CNBC Article:

* **The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.**
* **Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic.**
* **However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.**

The recent rise in bond yields and U.S. inflation expectations has some investors wary that a repeat of the 2013 “taper tantrum” could be on the horizon.

The benchmark U.S. 10-year Treasury note climbed above 1.3% for the first time since February 2020 earlier this week, while the 30-year bond also hit its highest level for a year. Yields move inversely to bond prices.

Yields tend to rise in lockstep with inflation expectations, which have reached their highest levels in a decade in the U.S., powered by increased prospects of a large fiscal stimulus package, progress on vaccine rollouts and pent-up consumer demand.

The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.

Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic. The Fed and others have maintained supportive tones in recent policy meetings, vowing to keep financial conditions loose as the global economy looks to emerge from the Covid-19 pandemic.

However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.

With central bank support removed, bonds usually fall in price which sends yields higher. This can also spill over into stock markets as higher interest rates means more debt servicing for firms, causing traders to reassess the investing environment.

“The supportive stance from policymakers will likely remain in place until the vaccines have paved a way to some return to normality,” said Shane Balkham, chief investment officer at Beaufort Investment, in a research note this week.

“However, there will be a risk of another ‘taper tantrum’ similar to the one we witnessed in 2013, and this is our main focus for 2021,” Balkham projected, should policymakers begin to unwind this stimulus.

Long-term bond yields in Japan and Europe followed U.S. Treasurys higher toward the end of the week as bondholders shifted their portfolios.

“The fear is that these assets are priced to perfection when the ECB and Fed might eventually taper,” said Sebastien Galy, senior macro strategist at Nordea Asset Management, in a research note entitled “Little taper tantrum.”

“The odds of tapering are helped in the United States by better retail sales after four months of disappointment and the expectation of large issuance from the $1.9 trillion fiscal package.”

Galy suggested the Fed would likely extend the duration on its asset purchases, moderating the upward momentum in inflation.

“Equity markets have reacted negatively to higher yield as it offers an alternative to the dividend yield and a higher discount to long-term cash flows, making them focus more on medium-term growth such as cyclicals” he said. Cyclicals are stocks whose performance tends to align with economic cycles.

Galy expects this process to be more marked in the second half of the year when economic growth picks up, increasing the potential for tapering.

## Tapering in the U.S., but not Europe

Allianz CEO Oliver Bäte told CNBC on Friday that there was a geographical divergence in how the German insurer is thinking about the prospect of interest rate hikes.

“One is Europe, where we continue to have financial repression, where the ECB continues to buy up to the max in order to minimize spreads between the north and the south — the strong balance sheets and the weak ones — and at some point somebody will have to pay the price for that, but in the short term I don’t see any spike in interest rates,” Bäte said, adding that the situation is different stateside.

“Because of the massive programs that have happened, the stimulus that is happening, the dollar being the world’s reserve currency, there is clearly a trend to stoke inflation and it is going to come. Again, I don’t know when and how, but the interest rates have been steepening and they should be steepening further.”

## Rising yields a ‘normal feature’

However, not all analysts are convinced that the rise in bond yields is material for markets. In a note Friday, Barclays Head of European Equity Strategy Emmanuel Cau suggested that rising bond yields were overdue, as they had been lagging the improving macroeconomic outlook for the second half of 2021, and said they were a “normal feature” of economic recovery.

“With the key drivers of inflation pointing up, the prospect of even more fiscal stimulus in the U.S. and pent up demand propelled by high excess savings, it seems right for bond yields to catch-up with other more advanced reflation trades,” Cau said, adding that central banks remain “firmly on hold” given the balance of risks.

He argued that the steepening yield curve is “typical at the early stages of the cycle,” and that so long as vaccine rollouts are successful, growth continues to tick upward and central banks remain cautious, reflationary moves across asset classes look “justified” and equities should be able to withstand higher rates.

“Of course, after the strong move of the last few weeks, equities could mark a pause as many sectors that have rallied with yields look overbought, like commodities and banks,” Cau said.

“But at this stage, we think rising yields are more a confirmation of the equity bull market than a threat, so dips should continue to be bought'''

len(txt)

# Import the BertTokenizer and BertModel classes from the transformers library
from transformers import BertTokenizer, BertModel

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# `BertTokenizer` is used to convert text into token IDs that the BERT model can process.
# 'bert-base-uncased' is a pre-trained BERT model that has been fine-tuned on uncased English text.
# The uncased version means that the tokenizer does not differentiate between uppercase and lowercase letters.
# The tokenizer handles tasks like tokenization (splitting text into tokens) and converting tokens into input IDs for the model.

# Initialize the BERT model
model = BertModel.from_pretrained('bert-base-uncased')
# `BertModel` is the core model class for BERT (Bidirectional Encoder Representations from Transformers).
# 'bert-base-uncased' is a pre-trained BERT model with 12 layers, 768 hidden units, and 110 million parameters.
# It provides contextualized embeddings for input tokens, capturing semantic meaning from the context.
# The model outputs hidden states for each token in the input sequence, which can be used for various NLP tasks.

# Example usage:
# - Tokenize text: tokenizer.encode("Hello world")
# - Process text with the model: model(input_ids)

# text = "This is a long text that needs to be encoded and decoded."
# Encode the text using the BERT tokenizer
encoded_text = tokenizer.encode(txt, padding=True, truncation=True, return_tensors='pt')
# `tokenizer.encode` processes the input text (`txt`) and converts it into token IDs that can be fed into the BERT model.
# Parameters:
# - `txt`: The input text to be tokenized and encoded.
# - `padding=True`: Ensures that all sequences are padded to the maximum length of the batch. This is required for batch processing where all inputs need to be the same length.
# - `truncation=True`: Truncates the input text to the maximum length allowed by the model. This is necessary to handle texts that exceed the model's maximum input length.
# - `return_tensors='pt'`: Converts the output into PyTorch tensors. This is useful for directly passing the encoded inputs to a PyTorch-based model.
#
# The result, `encoded_text`, is a PyTorch tensor containing token IDs, attention masks, and possibly other inputs required by the BERT model.

encoded_text.shape

# Decode the token IDs back into human-readable text
decoded_text = tokenizer.decode(encoded_text[0], skip_special_tokens=True)
# `tokenizer.decode` converts the token IDs in `encoded_text[0]` back into a string of text.
# Parameters:
# - `encoded_text[0]`: The first element of the `encoded_text` tensor, which contains the token IDs that need to be decoded.
# - `skip_special_tokens=True`: Excludes special tokens (such as [CLS], [SEP], [PAD], etc.) from the output text. This makes the decoded text more readable by removing tokens that are not part of the actual content.
#
# The result, `decoded_text`, is a string of human-readable text reconstructed from the token IDs.

decoded_text

len(decoded_text)

"""Let's Keyword Search using TF-IDF approach

# TF-IDF: A Simple Explanation

## What is TF-IDF?

TF-IDF stands for "Term Frequency-Inverse Document Frequency." It's a way to figure out how important a word is in a document or a collection of documents.

## Breaking it Down

### TF (Term Frequency)

- This measures how often a word appears in a document.
- The more a word appears, the more important it might be.

**Example:**
In the sentence "The cat sat on the mat," the word "the" appears twice, while "cat" appears once.

### IDF (Inverse Document Frequency)

- This measures how common or rare a word is across all documents.
- Rare words are often more important or informative.

**Example:**
If you have 1000 documents about animals, and "cat" appears in 100 of them, while "aardvark" appears in only 5, "aardvark" would have a higher IDF.

## Putting TF and IDF Together

TF-IDF combines these two measures:
- It gives a high score to words that appear frequently in a document but are rare across all documents.
- It gives a low score to words that appear in many documents or appear rarely in a document.

## Real-World Example

Imagine you have a collection of news articles:

1. An article about a football match might have high TF-IDF scores for words like "goal," "referee," and player names.
2. An article about cooking might have high TF-IDF scores for words like "recipe," "ingredients," and specific dish names.

Common words like "the," "and," or "is" would have low TF-IDF scores in both articles because they appear frequently in almost all documents.

## Why is TF-IDF Useful?

1. **Search Engines**: It helps rank search results by identifying the most relevant documents.
2. **Content Recommendation**: It can find similar articles or documents.
3. **Text Summarization**: It can identify the most important words or sentences in a document.
4. **Spam Detection**: It can help identify unusual word patterns that might indicate spam.

## Conclusion

TF-IDF is a simple but powerful tool that helps us understand the importance of words in documents. It balances how often a word appears in a document with how unique it is across all documents, giving us valuable insights into text data.
"""

# List of documents
documents = [
    "The cat is playing in the garden",
    "A dog and cat are good pets",
    "Cats love to chase mice",
    "machine learning is based on algorithms ",
    "Deep learning uses neural networks",
    "Recurrent networks have connections",
    "The cost of this shirt is $15"
]

# Tokenize documents into words

tokenized_docs = [word_tokenize(d) for d in documents]

# Inverted index mapping words to document IDs
index = {}
for i, doc in enumerate(tokenized_docs):
    for word in doc:
        if word not in index:
            index[word] = []
        index[word].append(i)

print(f'Inverted Index:\n {index}')

# Compute TF-IDF weights for words
def tfidf(word, id):
    # Term frequency - no. of times word appears in document
    tf = tokenized_docs[id].count(word)

    # No. of documents containing the word
    df = len(index[word]) if word in index else 0

    # Inverse document frequency
    idf =  math.log(len(documents) / (df+1))

    return tf * idf

# Search query
query = "animal that chases mouse"
query_words = word_tokenize(query)

# Initialize a dictionary to store scores for each document
scores = {id: 0 for id in range(len(documents))}
# `scores` is a dictionary where each key is a document ID (from 0 to len(documents)-1) and each value is initialized to 0.
# This dictionary will be used to accumulate scores for each document based on query words.

# Iterate over each query word
for q in query_words:
    # Check if the query word `q` exists in the index
    if q in index:
        # If the query word exists in the index, process each document ID associated with the query word
        for id in index[q]:
            # Increment the score for the document with ID `id` by the TF-IDF score of the query word `q`
            scores[id] += tfidf(q, id)

# `scores` now contains the accumulated TF-IDF scores for each document based on the query words
# This dictionary can be used to rank documents or retrieve the most relevant ones

# Rank documents based on their scores
ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
# `scores.items()` returns a view of the dictionary's items (document ID and score pairs).
# `sorted(..., key=lambda x: x[1], reverse=True)` sorts these items:
# - `key=lambda x: x[1]` sorts by the score (second element of each pair).
# - `reverse=True` ensures that the highest scores come first, ranking documents from most to least relevant.

# Print the ranked documents and their scores
for id, score in ranked:
    # Print the document ID and its content
    print(f"Document {id}: {documents[id]}")
    # Print the score associated with the document
    print(f"Score: {score}")

"""Semantic Search"""

# Import the SentenceTransformer class from the sentence_transformers library
from sentence_transformers import SentenceTransformer
# `SentenceTransformer` is a class used for obtaining sentence embeddings using pre-trained models.
# These embeddings represent sentences or texts in a high-dimensional vector space, capturing semantic meaning.

# Import the spatial module from the scipy library
import scipy.spatial
# The `scipy.spatial` module provides functions for spatial operations and distance calculations.
# This includes functions for computing distance metrics such as Euclidean distance, cosine distance, and others.

# Import the pandas library as pd
import pandas as pd
# `pandas` is a powerful data manipulation and analysis library.
# It provides data structures like DataFrames for efficiently handling and analyzing structured data.

documents = [
    "The cat is playing in the garden",
    "A dog and cat are good pets",
    "Cats love to chase mice",
    "Machine learning is based on algorithms",
    "Deep learning uses neural networks",
    "Recurrent networks have connections",
     "The cost of this shirt is $15"
]

# Initialize the SentenceTransformer model with a pre-trained model and move it to the GPU
model = SentenceTransformer('all-MiniLM-L6-v2').to('cuda')
# `SentenceTransformer('all-MiniLM-L6-v2')`:
# - Loads a pre-trained SentenceTransformer model named 'all-MiniLM-L6-v2'.
# - This model is part of the MiniLM family, which is designed to provide efficient and high-quality embeddings.
#
# Model Details:
# - **Architecture**: MiniLM (Miniature Language Model).
# - **Layers**: 6 transformer layers.
# - **Hidden Size**: 384 hidden units (each layer).
# - **Attention Heads**: 12 attention heads.
# - **Training Data**: Trained on a diverse range of text from various sources to generalize well across different NLP tasks.
# - **Performance**: Balances speed and accuracy. Provides good performance for generating sentence embeddings while being computationally efficient.
# - **Use Cases**: Suitable for tasks like semantic textual similarity, sentence clustering, and retrieval tasks.
#
# `.to('cuda')`

print(model)

# Generate embeddings for the provided documents and move them to the CPU
embeddings = model.encode(documents, convert_to_tensor=True).to('cpu')
# `model.encode(documents, convert_to_tensor=True)`:
# - `documents`: A list of text documents or sentences for which embeddings are to be generated.
# - `model.encode()`: This method takes the input text and computes embeddings using the pre-trained model.
# - `convert_to_tensor=True`: Converts the resulting embeddings into a PyTorch tensor.
#   - PyTorch tensors are used for efficient computation and manipulation within the PyTorch framework.
# - The embeddings are high-dimensional vectors representing the semantic meaning of each document.
#
# `.to('cpu')`:
# - Moves the tensor of embeddings to the CPU.
# - This is often done after computation on a GPU to ensure compatibility with other operations or save memory if further processing is done on the CPU.
# - If the model was run on a GPU but you want to perform further computations or storage on the CPU, you need to transfer the tensor back to the CPU.

embeddings.shape

embeddings

# User query
query = "machine learning"

# Encode query
query_embedding = model.encode(query)

query_embedding.shape

# Compute cosine similarity scores between the query embedding and each document embedding
scores = [spatial.distance.cosine(query_embedding, doc) for doc in embeddings]
# `spatial.distance.cosine(query_embedding, doc)`:
# - Computes the cosine distance between two vectors: `query_embedding` and `doc`.
# - `query_embedding`: A high-dimensional vector representing the query or search input.
# - `doc`: A high-dimensional vector representing a document's embedding.
# - Cosine distance is calculated as `1 - cosine similarity`, where cosine similarity measures the cosine of the angle between two vectors.
#   - This metric is useful for measuring the dissimilarity between vectors.
#   - A smaller cosine distance indicates higher similarity between the query and document embeddings.
#
# `[spatial.distance.cosine(query_embedding, doc) for doc in embeddings]`:
# - A list comprehension that iterates over each document embedding in the `embeddings` list.
# - For each document, it calculates the cosine distance between the `query_embedding` and the document's embedding.
# - Stores all computed distances in the `scores` list.
# - `scores` will be a list of cosine distances representing how similar each document is to the query.

scores

# Compute cosine distances between the query embedding and each document embedding
scores = [scipy.spatial.distance.cosine(query_embedding, doc) for doc in embeddings]
# `scipy.spatial.distance.cosine(query_embedding, doc)`:
# - Computes the cosine distance between two vectors: `query_embedding` and `doc`.
# - `query_embedding`: A high-dimensional vector representing the query or search input.
# - `doc`: A high-dimensional vector representing a document's embedding.
# - Cosine distance measures dissimilarity, where a lower value indicates higher similarity.
# - This list comprehension calculates the cosine distance for each document embedding and stores these values in `scores`.

# Print each score along with its corresponding document
for i, doc in enumerate(documents):
    # Print the document's index and its content
    print(f"Document {i}: {documents[i]}")
    # Print the cosine similarity score for the document
    # `1 - scores[i]` converts the cosine distance to cosine similarity, where higher values indicate more similarity
    print(f"Score: {1 - scores[i]:.4f}")

# Identify and print the most similar document based on the lowest cosine distance
# `min(scores)` finds the smallest cosine distance, indicating the most similar document
print("Most similar document:", documents[scores.index(min(scores))])

def semantic_search(query, embeddings, documents):
    # Generate embedding for the query text
    query_embedding = model.encode(query)
    # `model.encode(query)`:
    # - Encodes the input query text into a high-dimensional vector (embedding) using the pre-trained model.
    # - This vector represents the semantic meaning of the query.

    # Document embeddings
    document_embeddings = embeddings
    # `embeddings` should be a list or array of pre-computed embeddings for the documents.

    # Compute cosine distances between the query embedding and each document embedding
    scores = [scipy.spatial.distance.cosine(query_embedding, doc) for doc in document_embeddings]
    # `scipy.spatial.distance.cosine(query_embedding, doc)`:
    # - Computes the cosine distance (dissimilarity) between the query embedding and each document embedding.
    # - The list comprehension calculates this distance for every document embedding and stores these distances in `scores`.

    # Print similarity scores for each document
    for i, doc in enumerate(documents):
        # Print each document along with its similarity score
        # `1 - scores[i]` converts cosine distance to cosine similarity, where higher values indicate higher similarity
        print(f"{doc}: {1 - scores[i]:.4f}")

    # Identify and print the most similar document
    most_similar_doc = documents[scores.index(min(scores))]
    # `min(scores)` finds the smallest cosine distance, indicating the highest similarity.
    # `scores.index(min(scores))` retrieves the index of this smallest distance.
    # `documents[scores.index(min(scores))]` gets the document corresponding to this index.
    print("Most similar document:", most_similar_doc)

semantic_search("an animal that chases mouse",embeddings,documents)

semantic_search("The cost of this shirt is $1099",embeddings,documents)

