# -*- coding: utf-8 -*-
"""[Stanford]LLM-v2-module-5_large_embeddingv1.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16_IbpSeMjsLS6QKM6VP3GiSffmnf2ML_
"""

# Commented out IPython magic to ensure Python compatibility.
# Install the 'sentence_transformers' library, which provides pre-trained models for sentence embeddings.
!pip install sentence_transformers openai unstructured

# Install 'plotly', a library for creating interactive visualizations.
!pip install plotly

# Upgrade 'langchain-openai' to the latest version. This library provides tools for using OpenAI's language models within the LangChain framework.
!pip install -U langchain-openai

# Install 'langchain_community', which may include additional tools or features for the LangChain framework.
!pip install langchain_community

# Import 'OpenAIEmbeddings' and 'ChatOpenAI' from 'langchain_openai' to use OpenAI's embeddings and chat functionalities within LangChain.
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI

# Install 'tiktoken', a library for tokenization which may be used with large language models.
!pip install tiktoken

# Install 'matplotlib', a library for creating static, animated, and interactive visualizations in Python.
!pip install matplotlib

# Install and upgrade several libraries including:
# - 'rich' for rich text and beautiful formatting in the console.
# - 'openai' for accessing OpenAI's API.
# - 'tiktoken' for tokenization.
# - 'wandb' for experiment tracking and management.
# - 'langchain' for language model tools and chaining.
# - 'unstructured' for handling unstructured data.
# - 'tabulate' for pretty-printing tabular data.
# - 'pdf2image' for converting PDFs to images.
# - 'chromadb' for working with ChromaDB.
# - 'gradio' for creating interactive web interfaces.
# - 'faiss-gpu' for efficient similarity search using GPU acceleration.
# %pip install -Uqqq rich openai tiktoken wandb langchain unstructured tabulate pdf2image chromadb gradio faiss-gpu

# Import the 'TextLoader' class from 'langchain_community.document_loaders'.
# This class is used to load text documents into a format suitable for processing with LangChain.
from langchain_community.document_loaders import TextLoader

# Import the 'FAISS' class from 'langchain_community.vectorstores'.
# FAISS is a library for efficient similarity search and clustering of dense vectors.
# It integrates with LangChain for vector-based search functionalities.
from langchain_community.vectorstores import FAISS

# Import 'OpenAIEmbeddings' from 'langchain_openai'.
# This class is used to generate embeddings for text using OpenAI's models within LangChain.
from langchain_openai import OpenAIEmbeddings

# Import 'CharacterTextSplitter' from 'langchain_text_splitters'.
# This class is used to split text documents into smaller chunks or segments, often for processing or embedding.
from langchain_text_splitters import CharacterTextSplitter

# Import 'PromptTemplate' from 'langchain_core.prompts'.
# This class is used to create and manage prompt templates for interacting with language models.
from langchain_core.prompts import PromptTemplate

# Import 'LLMChain' from 'langchain.chains'.
# This class facilitates the chaining of multiple language model operations, enabling more complex workflows.
from langchain.chains import LLMChain

!pip install linkify-it-py

# Import the 'numpy' library and alias it as 'np'.
# NumPy is a fundamental package for scientific computing in Python, providing support for arrays, matrices, and many mathematical functions.
import numpy as np

# Import the 'os' and 'random' modules.
# 'os' provides a way to interact with the operating system, such as file handling and environment variables.
# 'random' provides functions for generating random numbers and performing random selections.
import os, random

# Import the 'Path' class from the 'pathlib' module.
# 'Path' provides an object-oriented interface for filesystem paths, making it easier to handle and manipulate file paths.
from pathlib import Path

# Import the 'tiktoken' module.
# 'tiktoken' is used for tokenizing text into smaller units, useful for processing and analyzing text data.
import tiktoken

# Import the 'getpass' function from the 'getpass' module.
# 'getpass' is used to securely prompt the user for a password or other sensitive input without echoing the input on the screen.
from getpass import getpass

# Import the 'Markdown' class from the 'rich.markdown' module.
# 'Markdown' is used to render Markdown text in a rich, formatted style within the console or terminal.
from rich.markdown import Markdown

# Import the 'load_qa_with_sources_chain' function from 'langchain.chains.qa_with_sources'.
# This function is used to load a question-answering chain that includes source documents, enabling the retrieval of answers based on specific sources.
from langchain.chains.qa_with_sources import load_qa_with_sources_chain

# Import the 'torch' library.
# PyTorch is a popular open-source deep learning framework that provides tools for building and training neural networks.
import torch

# Import the 'sys' module.
# 'sys' provides access to system-specific parameters and functions, such as interacting with the Python runtime environment.
import sys

# Import the 'csv' module.
# 'csv' provides functionality for reading from and writing to CSV (Comma-Separated Values) files.
import csv

# Set the maximum field size for CSV files to the maximum size of an integer on the system.
# This is useful when dealing with large CSV fields that may exceed the default size limit.
csv.field_size_limit(sys.maxsize)

# Check if the 'torch' library has support for Metal Performance Shaders (MPS) on Apple devices.
# MPS is a framework for high-performance GPU-accelerated computing on macOS. The line is commented out but could be used to verify MPS support.
# torch.has_mps

from google.colab import userdata

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")
#os.environ["OPENAI_API_KEY"] = ""

if os.getenv("OPENAI_API_KEY") is None:
  if any(['VSCODE' in x for x in os.environ.keys()]):
    print('Please enter password in the VS Code prompt at the top of your VS Code window!')
  os.environ["OPENAI_API_KEY"] = getpass("")

assert os.getenv("OPENAI_API_KEY", "").startswith("sk-"), "This doesn't look like a valid OpenAI API key"
print("OpenAI API key configured")

"""#Chunking and RAG"""

# from langchain.embeddings.openai import OpenAIEmbeddings
# from langchain.text_splitter import CharacterTextSplitter
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.vectorstores import Chroma
# from langchain.document_loaders import TextLoader
# from langchain.chat_models import ChatOpenAI
# from langchain import PromptTemplate
# from langchain.chains import LLMChain
# from langchain.chains.qa_with_sources import load_qa_with_sources_chain
# from langchain.llms import OpenAI
# from langchain.vectorstores import FAISS

from langchain_community.document_loaders.csv_loader import CSVLoader

MODEL_NAME = "text-embedding-3-small"
# MODEL_NAME = "gpt-4"

# from langchain.document_loaders import DirectoryLoader

# def find_md_files(directory):
#     "Find all markdown files in a directory and return a LangChain Document"
#     dl = DirectoryLoader(directory, "**/*.txt")
#     return dl.load()

# documents = find_md_files('data')
# len(documents)

# documents

"""#upload files"""

import pandas as pd

# from langchain.document_loaders.csv_loader import CSVLoader
# # from langchain.docstore.document import Document
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.document_loaders import DirectoryLoader

url='https://drive.google.com/file/d/1gl7WAkJr6Nyke7YckzXxdL-iM4UjhLGX/view?usp=sharing'
url='https://drive.google.com/uc?id=' + url.split('/')[-2]
df = pd.read_csv(url)

df.head()

df.shape

!mkdir data

df[:100].to_csv('data/df_embed.csv',index=False)



"""1. Step 1: Read the data"""

df.full_text	[1]

df.head()

path= 'data/df_embed.csv'
loader = CSVLoader(file_path=path,source_column="title")

data = loader.load()

print(data[0])

print (f'You have {len(data)} document(s) in your data')
print (f'There are {len(data[0].page_content)} words in your first document')

"""2. Breakdown data - Chunking Process"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-text-splitters

# Import the 'RecursiveCharacterTextSplitter' class from the 'langchain_text_splitters' module.
# This class is used to split text into chunks based on character count, with options for managing chunk size and overlap.
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Initialize the RecursiveCharacterTextSplitter with specific parameters
# 'chunk_size' specifies the maximum size of each text chunk, set to 1000 characters here.
# 'chunk_overlap' specifies the number of characters that should overlap between consecutive chunks, set to 100 here.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

# Use the text_splitter to split the 'data' into smaller chunks according to the specified parameters.
# 'data' is assumed to be a collection of documents or a large text that needs to be divided into smaller chunks.
# 'split_documents' is a method that processes the input data and returns a list of text chunks.
texts = text_splitter.split_documents(data)

len(texts)

print (f'You now have {len(texts)} document(s) in your data')
print (f'There are {len(texts[1].page_content)} characters in your document')

texts[0]

embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(texts, embeddings)

db.save_local("faiss_index")

"""#The new place to begin"""

# Load a FAISS index from a local file and initialize it with the specified embeddings
# 'FAISS.load_local' is used to load a previously saved FAISS index from disk.
# 'faiss_index' is the path to the file where the FAISS index is stored.
# 'embeddings' is the embeddings object that was used to create the FAISS index,
# which is required to interpret the vectors stored in the index.
# 'allow_dangerous_deserialization=True' allows loading of potentially unsafe data,
# which should be used with caution to avoid security risks.

db_loaded = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

query = """Why is it so bad for Democrats in Senate in 2024"""

def get_response_from_query(db, query, k=10):
    """
    Function to retrieve relevant documents from a database based on a query, generate a response using an LLM,
    and evaluate the response for its faithfulness.

    Parameters:
    - db: The database object used for similarity search.
    - query: The query string to search for in the database.
    - k: The number of top documents to retrieve (default is 10).

    Returns:
    - response: The generated response from the LLM.
    - docs: The list of retrieved documents.
    - evals: Evaluation of the response for faithfulness.
    """

    # Perform a similarity search in the database to retrieve the top 'k' documents relevant to the query.
    docs = db.similarity_search(query, k=k)  # Perform similarity search on the database

    # Combine the content of the retrieved documents into a single string.
    docs_page_content = " ".join([d.page_content for d in docs])

    # Initialize the language model for generating responses.
    # Using OpenAI's GPT-3.5-turbo model with a temperature of 0 for deterministic responses.
    llm = ChatOpenAI(model_name="gpt-3.5-turbo-16k", temperature=0)

    # Define the prompt template for generating responses.
    prompt = PromptTemplate(
        input_variables=["question", "docs", "giulio_variable"],
        template="""
        You are a bot that is open to discussions about different cultural, philosophical and political exchanges. I will use do different analysis to the articles provided to me. Stay truthful and if you weren't provided any resources give your oppinion only.
        Answer the following question: {question}
        By searching the following articles: {docs}
        Giulio variable: {giulio_variable}

        Only use the factual information from the documents. Make sure to mention key phrases from the articles.

        If you feel like you don't have enough information to answer the question, say "I don't know".

        """,
    )

    # Create a chain that combines the language model and the prompt template for generating responses.
    chain = LLMChain(llm=llm, prompt=prompt)

    # A variable to be included in the prompt, potentially for specific context or additional information.
    giulio_variable = 'oeifmeofjoegoe'

    # Generate a response to the query using the language model and the retrieved documents.
    response = chain.run(question=query, docs=docs_page_content, giulio_variable=giulio_variable, return_source_documents=True)
    r_text = str(response)  # Convert response to string

    # Define a prompt template for evaluating the faithfulness of the generated response.
    prompt_eval = PromptTemplate(
        input_variables=["answer", "docs"],
        template="""
        You job is to evaluate if the response to a given context is faithful.

        for the following: {answer}
        By searching the following article: {docs}

       Give a reason why they are similar or not, start with a Yes or a No.

        """,
    )

    # Create a chain for evaluating the response using the language model and the evaluation prompt.
    chain_part_2 = LLMChain(llm=llm, prompt=prompt_eval)

    # Evaluate the generated response for its faithfulness based on the retrieved documents.
    evals = chain_part_2.run(answer=r_text, docs=docs_page_content)

    # Return the generated response, the retrieved documents, and the evaluation of the response.
    return response, docs, evals

answer,sources,evals=get_response_from_query(db_loaded,query,15)

sources

from pprint import pprint

# Print question, answer, and evaluations
print("\n\n> Question:")
pprint(query)
print("\n> Answer:")
pprint(answer)
print("\n> Eval:")
pprint(evals)

# Print the relevant sources used for the answer
print("----------------------------------SOURCE DOCUMENTS---------------------------")
for document in sources:
    print("\n> " + document.metadata["source"])
    pprint(document.page_content[:1000])
print("----------------------------------SOURCE DOCUMENTS---------------------------")

import gradio as gr

def get_response_from_query(db, query, k=3):
    """
    Function to retrieve relevant documents from a database based on a query, generate a response using an LLM,
    and evaluate the response for its faithfulness.

    Parameters:
    - db: The database object used for similarity search.
    - query: The query string to search for in the database.
    - k: The number of top documents to retrieve (default is 3).

    Returns:
    - response: The generated response from the LLM.
    - docs: The list of retrieved documents.
    - evals: Evaluation of the response for faithfulness.
    """

    # Perform a similarity search in the database to retrieve the top 'k' documents relevant to the query.
    docs = db.similarity_search(query, k=k)

    # Combine the content of the retrieved documents into a single string.
    docs_page_content = " ".join([d.page_content for d in docs])

    # Initialize the language model for generating responses.
    # Using OpenAI's GPT-4o-mini model with a temperature of 0 for deterministic responses.
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    # Define the prompt template for generating responses.
    prompt = PromptTemplate(
        input_variables=["question", "docs"],
        template="""
        A bot that is open to discussions about different cultural, philosophical, and political exchanges. I will use different analyses of the articles provided to me. Stay truthful and if you weren't provided any resources, give your opinion only.
        Answer the following question: {question}
        By searching the following articles: {docs}

        Only use the factual information from the documents. Make sure to mention key phrases from the articles.

        If you feel like you don't have enough information to answer the question, say "I don't know".
        """,
    )

    # Create a chain that combines the language model and the prompt template for generating responses.
    chain = LLMChain(llm=llm, prompt=prompt)

    # Generate a response to the query using the language model and the retrieved documents.
    response = chain.run(question=query, docs=docs_page_content, return_source_documents=True)
    r_text = str(response)  # Convert response to string

    # Define a prompt template for evaluating the faithfulness of the generated response.
    prompt_eval = PromptTemplate(
        input_variables=["answer", "docs"],
        template="""
        Your job is to evaluate if the response to a given context is faithful.

        For the following: {answer}
        By searching the following article: {docs}

        Give a reason why they are similar or not, starting with a Yes or a No.
        """,
    )

    # Create a chain for evaluating the response using the language model and the evaluation prompt.
    chain_part_2 = LLMChain(llm=llm, prompt=prompt_eval)

    # Evaluate the generated response for its faithfulness based on the retrieved documents.
    evals = chain_part_2.run(answer=r_text, docs=docs_page_content)

    # Return the generated response, the retrieved documents, and the evaluation of the response.
    return response, docs, evals

def greet(query):

    answer,sources,evals = get_response_from_query(db,query,2)
    return answer,sources,evals
examples = [
    ["How to be happy"],
    ["Climate Change Challenges in Europe"],
    ["Philosophy in the world of Minimalism"],
    ["Hate Speech  vs Freedom of Speech"],
    ["Articles by Noam Chomsky on US Politics"],
    ["The importance of values and reflection"]
    ]
demo = gr.Interface(fn=greet, title="cicero-semantic-search", inputs="text",
                    outputs=[gr.components.Textbox(lines=3, label="Response"),
                             gr.components.Textbox(lines=3, label="Source"),
                             gr.components.Textbox(lines=3, label="Evaluation")],
                   examples=examples)

demo.launch(share=True)

from gradio_client import Client

client = Client("https://a39443a44ab5a4a382.gradio.live")
result = client.predict(
		"Can you tell me about the concerns of Democrats for 2024 Senate",	# str  in 'query' Textbox component
		api_name="/predict"
)
print(result)

