# -*- coding: utf-8 -*-
"""Module_3_Foundation_FAISS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuAEC8EfEek_Dvpai_SnCeqSphaqSHao

#Read Data
"""
from gettext import install

import requests
import os

data_url = "https://raw.githubusercontent.com/jamescalam/data/main/sentence_embeddings_15K/"
s=[]
# create data directory to store data
if not os.path.exists('./data'):
    os.mkdir('./data')

# download the numpy binary files (dense vectors)
for i in range(57):
    if i < 10:
        i = '0' + str(i)
    res = requests.get(data_url+f"embeddings_{i}.npy")

    with open(f'./data/embeddings_{i}.npy', 'wb') as fp:
        for chunk in res:
            fp.write(chunk)

    print('.', end='')

# and download the respective text file
res = requests.get(f"{data_url}sentences.txt")
with open(f"./data/sentences.txt", 'wb') as fp:
    for chunk in res:
        fp.write(chunk)

urls = [
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.train.tsv',
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.test.tsv',
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/OnWN.test.tsv',
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2013/OnWN.test.tsv',
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/OnWN.test.tsv',
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/images.test.tsv',
    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/images.test.tsv'
]

import pandas as pd

import requests
from io import StringIO
res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')
# create dataframe
data = pd.read_csv(StringIO(res.text), sep='\t')
data.head()

# we take all samples from both sentence A and B
sentences = data['sentence_A'].tolist()
sentences[:5]

# we take all samples from both sentence A and B
sentences = data['sentence_A'].tolist()
sentence_b = data['sentence_B'].tolist()
sentences.extend(sentence_b)  # merge them
len(set(sentences))  # together we have ~4.5K unique sentences

sentences[:5]

# each of these dataset have the same structure, so we loop through each creating our sentences data
for url in urls:
    res = requests.get(url)
    # extract to dataframe

    smalldata = pd.read_csv(StringIO(res.text), sep='\t', header=None, on_bad_lines='skip')
    # data=data.append(smalldata,ignore_index=False)
    # add to columns 1 and 2 to sentences list
    sentences.extend(smalldata[1].tolist())
    sentences.extend(smalldata[2].tolist())

# smalldata.head()

# remove duplicates and NaN
sentences = [word for word in list(set(sentences)) if type(word) is str]

len(sentences)

sentences[:5]

import numpy as np
!pip install faiss-gpu #cpu

"""# raw text > into Embeddings [using an encoder] > save it in FAISS > raw text query > into an Embedding > Euclidean distance to retrieve the text closest to the query"""

!pip install -U sentence-transformers

# !git clone https://github.com/jamescalam/data.git data-embeddings

# path = '/content/data-embeddings/sentence_embeddings_15K/'
# sentence_embeddings = []
# for i in range(0,57):
#     # if i < 10:
#     #     i = '0' + str(i)
#     res = path+f"embeddings_{i}.npy"
#     print(res)
#     sm = np.load(res)
#     sentence_embeddings.append(sm)

# arr = np.concatenate(sentence_embeddings)

# arr.shape

# sentences = sentences[:100]

from sentence_transformers import SentenceTransformer
# initialize sentence transformer model
#model = SentenceTransformer('bert-base-nli-mean-tokens') #encoding model
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

len(sentences)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sentence_embeddings = model.encode(sentences)
# sentence_embeddings.shape

sentences[0]



sentence_embeddings[0]

# with open(f'./sim_sentences/embeddings_X.npy', 'wb') as fp:
#     np.save(fp, sentence_embeddings[0:256])

# # saving data
# split = 256
# file_count = 0
# for i in range(0, sentence_embeddings.shape[0], split):
#     end = i + split
#     if end > sentence_embeddings.shape[0] + 1:
#         end = sentence_embeddings.shape[0] + 1
#     file_count = '0' + str(file_count) if file_count < 0 else str(file_count)
#     with open(f'./sim_sentences/embeddings_{file_count}.npy', 'wb') as fp:
#         np.save(fp, sentence_embeddings[i:end, :])
#     print(f"embeddings_{file_count}.npy | {i} -> {end}")
#     file_count = int(file_count) + 1

import faiss
# sentence_embeddings = arr

d = sentence_embeddings.shape[1]
d

sentences[0]

sentence_embeddings[0].shape

faiss.Index

index = faiss.IndexFlatL2(d) #euclidean method.  - intialize



index.is_trained

index.add(sentence_embeddings)

index.ntotal

"""converted raw text into embeddings > saved the embeddings into FAISS and we are going to use Euclidean distance to measure the distances"""

sentences[100:101]

k = 10
xq = model.encode(["Someone is performing a dance admidst the rainfall."])

"""Converted the query into an embedding and now we will run Euclidean search on the entire corpus and compare it to the query"""

# index

# Commented out IPython magic to ensure Python compatibility.
# %%time
# D, I = index.search(xq, k)  # search
# print(I)

[f'{i}: {sentences[i]}' for i in I[0]]

"""# Quantization"""

nlist = 100
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFFlat(quantizer, d, nlist)

index.is_trained

index.train(sentence_embeddings)
index.is_trained

index.add(sentence_embeddings)
index.ntotal

# Commented out IPython magic to ensure Python compatibility.
# %%time
# D, I = index.search(xq, k)  # search
# print(I)

index.nprobe = 10

# Commented out IPython magic to ensure Python compatibility.
# %%time
# D, I = index.search(xq, k)  # search
# print(I)

"""['582: A person is dancing in the rain',
 '7999: A man is dancing in the rain',
 '14299: A woman is performing in the rain',
 '8088: The dancer is dancing in front of the sound equipment']
"""

[f'{i}: {sentences[i]}' for i in I[0]]

"""#Homework: implement all the versions mentioned in this article: Comprehensive Guide To Approximate Nearest Neighbors Algorithms | by Eyal Trabelsi | Towards Data Science"""

index.make_direct_map()

"""Technique > Time > Subjective performance"""

